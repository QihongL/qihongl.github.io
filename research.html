<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Qihong Lu's research</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Qihong Lu</h1>
        <p></p>
        
                
        <p class="view"> <a href="index.html">Home</a></p>
        <p class="view">Research Projects</p>        
        <p class="view"> <a href="academicCV.html">Academic CV</a></p>
        
        <p class="view">
        <a href="https://github.com/QihongL"> <img src="myImages/logo/github.png" alt="myGithub" style="width:40px;height:40px;"></a>
        <a href="mailto:qihong.lu@wisc.edu" target="_top"><img src="myImages/logo/email.png" alt="myEmail" style="width:40px;height:40px;"></a>
        </p>
        
        <p class="view">
        <a href="https://www.linkedin.com/in/qihong"> <img src="myImages/logo/linkedin.png" alt="myLinkedin" style="width:40px;height:40px;"></a>
        <a href="https://www.facebook.com/qihong.lu.9"> <img src="myImages/logo/facebook.png" alt="myFacebook" style="width:40px;height:40px;"></a>        
        </p>

      </header>
      <section>
        <h3>


<h3><p> <strong>1. A recurrent neural network for human object recognition</strong></p> </h3>

[
<a href = "myDocs/cogsci2016_poster.pdf"> CogSci 2016 Poster</a>, 
<a href = "https://mindmodeling.org/cogsci2016/papers/0508/index.html"> CogSci 2016 Abstract</a>, 
<a href = "https://github.com/QihongL/categorization_PDP"> Code</a> 
]

<p style="width: 500px;">
<img src="/myImages/research/pdp_cat_demo.png">
</p> 
<p> This is a computational model for the temporal dynamics of human object recognition. Ultra-rapid categorization (<a href = "http://www.mitpressjournals.org/doi/abs/10.1162/jocn_a_00701#.V6ENc5MrJgo">Wu, et al., 2015</a>) has been considered as evidence for a feed-forward view of visual recognition (<a href = "http://www.pnas.org/content/104/15/6424.full">Serre, Oliva & Poggio, 2007</a>). Here, we show that an interactive model can be consistent with this result, and it can also explain many other behavioral and neural data. 
</p>
---
<p style="margin-left:.5in;text-indent:-.5in"> 
<b>Lu, Q.</b>, Cox, C., Rogers, T. T., Lambon Ralph, M.A., Takahashi R. (manuscript in preparation). An interactive account for human vision: a recurrent neural network explains neural and behavioral temporal dynamics of object recognition process.
</p>

<p style="margin-left:.5in;text-indent:-.5in"> 
<b>Lu, Q.</b>, & Rogers, T. T. (2016). An interactive model accounts for both ultra-rapid superordinate classification and basic-level advantage in object recognition. 
<i>
Poster presented at the 38th Annual Meeting of the Cognitive Science Society, Philadelphia, PA. 
</i>
</p>

<br><br><br>



<h3><p> <strong>2. A reinforcement learning network for "counting"</strong></p></h3>

[
<a href = "myDocs/ncpw16_poster.pdf">NCPW 2016 Poster</a>, 
<a href = "myDocs/Lu_McClelland_Teaching_ANN_to_count">NCPW 2016 Abstract</a>, 
<a href = "https://github.com/QihongL/mathCognition_PDP_RL"> Code </a> 
]

<p style="width: 400px;">
<img src="/myImages/research/countModel_envir.png">
</p> 

<p>Counting skill is a foundation for more sophisticated math concepts, and it takes children several years to learn to do it well. To understand this learning process, we explored how social feedback helps an artificial agent to learn a counting-related task. This modeling framework is inspired by the Deep-Q network (<a href = "http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html">Mnih, et al., 2015</a>). 

Currently, we are trying to augment this model with hard attention (<a href = "http://arxiv.org/abs/1406.6247">Mnih et al., 2014</a>).
</p>

---
<p style="margin-left:.5in;text-indent:-.5in"> 
<b>Lu, Q.</b>, & McClelland, J.L. (2016). Teaching a neural network to count: reinforcement learning with “social scaffolding”. 
<i>
Poster presented at the 15th Neural Computation and Psychology Workshop, Philadelphia, PA. 
</i>
</p>


<br><br><br>




<h3><p> <strong>3. Discover distributed representation via fMRI decoding</strong></p></h3>
</i>
[
<a href = "myDocs/cox_lu_rogers_CNS2015.pdf">CNS 2015 Poster </a>
,
<a href = "https://github.com/crcox/iterativelasso"> Code</a>
]
</p>

<p style="width: 400px;">
<img src="/myImages/research/ilassoResults.png">

<p>We investigated how faces, places and objects are represented in the human brain, with a novel multi-voxel pattern analysis (MVPA) method: whole-brain iterative logistic Lasso. Different from some conventional findings (<a href = "http://www.jneurosci.org/content/17/11/4302.full">Kanwisher, McDermott, and Chun, 1997</a>), we found that the neural representation of faces is highly distributed across the entire brain. </p>

---
<p style="margin-left:.5in;text-indent:-.5in"> 
Cox, C. R., <b>Lu, Q.</b>, & Rogers, T. T. (2015). Iterative Lasso: An even-handed approach to whole brain multivariate pattern analysis. 
<i>
Poster presented at the 22nd Cognitive Neuroscience Society Annual Conference, San Francisco, CA. 
</i>
<br>
</p>

<p style="margin-left:.5in;text-indent:-.5in"> 
Cox, C. R.,  <b>Lu, Q. </b>, & Rogers, T. T. (2015). Iterative Lasso: An even-handed approach to whole brain multivariate pattern analysis. 
<i>
Poster presented at the Neuroimaging, Computational Neuroscience and Neuroengineering Workshop, Madison, WI.
</p>


      </section>
      <footer>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
