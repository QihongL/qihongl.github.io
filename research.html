<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

<link rel="shortcut icon" href="https://www.cs.wisc.edu/sites/all/themes/cs_subtheme/favicon.ico" type="image/vnd.microsoft.icon" />
    
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-83024954-2', 'auto');
  ga('send', 'pageview');

</script>

    <title>Qihong Lu | Research</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Qihong Lu</h1>
        <p></p>
        
        <br>                
        <p class="view"> <a href="index.html">Home</a></p>
        <p class="view">Research Projects</p>        
        <p class="view"> <a href="academicCV.html">Academic CV</a></p>
        <p class="view"> <a href="tutorial.html">Tutorials</a></p>
        <p class="view"> <a href="resources.html">Resources</a></p>
        <br>
        
        <p class="view">
        <a href="https://github.com/QihongL"> <img src="myImages/logo/github.png" alt="myGithub" style="width:40px;height:40px;"></a>
        <a href="mailto:qihong.lu@princeton.edu" target="_top"><img src="myImages/logo/email.png" alt="myEmail" style="width:40px;height:40px;"></a>
        </p>
        
        <p class="view">
        <a href="https://www.linkedin.com/in/qihong"> <img src="myImages/logo/linkedin.png" alt="myLinkedin" style="width:40px;height:40px;"></a>
        <a href="https://www.facebook.com/qihong.lu.9"> <img src="myImages/logo/facebook.png" alt="myFacebook" style="width:40px;height:40px;"></a>        
        </p>
        
        <p class="view">
        <a href="https://twitter.com/Qihong_Lu"> <img src="myImages/logo/twitter_flat.png" alt="myTwitter" style="width:42px;height:42px;"></a>
<!--         <a href="https://www.facebook.com/qihong.lu.9"> <img src="myImages/logo/facebook.png" alt="myFacebook" style="width:40px;height:40px;"></a>         -->
        </p>
        
      </header>
      <section>
        <h3>

<h3 id="top"><p> <strong>1. A recurrent neural network for human object recognition</strong></p> </h3>
<center>
<p style="width: 500px;">
<img src="/myImages/research/pdp_cat_demo.png"></p></center>
<p> The speed of ultra-rapid categorization (<a href = "http://www.mitpressjournals.org/doi/abs/10.1162/jocn_a_00701#.V6ENc5MrJgo">Wu, et al., 2015</a>) has been considered as evidence for a feed-forward view of visual recognition (<a href = "http://www.pnas.org/content/104/15/6424.full">Serre, Oliva & Poggio, 2007</a>). Here, we built a recurrent neural network that captures the behavioral and neural temporal dynamics of visual recognition processes, including the rapid recognition data and several other empirical patterns. These results provide evidence that object recognition is supported by interactive processes in the brain.
<br>
- [
<a href = "myDocs/cogsci2016_poster.pdf"> CogSci 2016 Poster</a>, 
<a href = "https://mindmodeling.org/cogsci2016/papers/0508/index.html"> CogSci 2016 Abstract</a>, 
<a href = "https://github.com/QihongL/categorization_PDP"> Code</a> 
]
</p>


<p style="margin-left:.5in;text-indent:-.5in"> 
<b>Lu, Q.</b>, Cox, C., Rogers, T. T., Lambon Ralph, M.A., Takahashi R. (manuscript in preparation). An interactive account for human vision: a recurrent neural network explains neural and behavioral temporal dynamics of object recognition process.
</p>

<p style="margin-left:.5in;text-indent:-.5in"> 
<b>Lu, Q.</b>, & Rogers, T. T. (2016). An interactive model accounts for both ultra-rapid superordinate classification and basic-level advantage in object recognition. 
<i>
Poster presented at the 38th Annual Meeting of the Cognitive Science Society, Philadelphia, PA. 
</i>
</p>

<br><br><br>



<h3><p> <strong>2. A reinforcement learning network for "counting"</strong></p></h3>

<center>
<p style="width: 450px;">
<img src="/myImages/research/countModel_envir.png"></p></center>

<p>
Counting skill is a foundation for more sophisticated math concepts, and it takes children several years to learn to do it well. To understand this learning process, we explored how social feedback helps an artificial agent to learn a counting-related task. Our modeling framework is inspired by the Deep-Q network (<a href = "http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html">Mnih, et al., 2015</a>). 

Currently, I am trying to <a href = "https://github.com/QihongL/mathCognition_RAM"> augment this idea with hard attention</a> (<a href = "http://arxiv.org/abs/1406.6247">Mnih et al., 2014</a>).
<br>
- [
<a href = "myDocs/ncpw16_poster.pdf">NCPW 2016 Poster</a>, 
<a href = "myDocs/Lu_McClelland_Teaching_ANN_to_count.pdf">NCPW 2016 Abstract</a>, 
<a href = "https://github.com/QihongL/mathCognition_PDP_RL"> Code </a> 
]
</p>



<p style="margin-left:.5in;text-indent:-.5in"> 
<b>Lu, Q.</b>, & McClelland, J.L. (2016). Teaching a neural network to count: reinforcement learning with “social scaffolding”. 
<i>
Poster presented at the 15th Neural Computation and Psychology Workshop, Philadelphia, PA. 
</i>
</p>


<br><br><br>




<h3><p> <strong>3. Discover distributed representation with sparse MVPA methods</strong></p></h3>
</i>
</p>

<center>
<p style="width: 450px;">
<img src="/myImages/research/ilassoResults.png"></p></center>

<p>
We investigated the localization of the neural representations of faces, places, and objects. We developed a novel sparse multi-voxel pattern analysis (MVPA) method, which identifies a subset of brain regions (voxels) that predict the kind of pictures (e.g. face vs. non-face) presented to the participants, given their fMRI data. Besides some classic ROIs (e.g. <a href = "http://www.jneurosci.org/content/17/11/4302.full">Kanwisher, McDermott, and Chun, 1997</a>), we also found a bunch of "extra" brain regions that are distributed, signal-carrying and idiosyncratic across subjects. 

<br>
- [
<a href = "myDocs/cox_lu_rogers_CNS2015.pdf">CNS 2015 Poster </a>
,
<a href = "https://github.com/crcox/iterativelasso"> Code</a>
]
</p>

<p style="margin-left:.5in;text-indent:-.5in"> 
Cox, C. R., <b>Lu, Q.</b>, & Rogers, T. T. (2015). Iterative Lasso: An even-handed approach to whole brain multivariate pattern analysis. 
<i>
Poster presented at the 22nd Cognitive Neuroscience Society Annual Conference, San Francisco, CA. 
</i>
<br>
</p>

<p style="margin-left:.5in;text-indent:-.5in"> 
Cox, C. R.,  <b>Lu, Q.</b>, & Rogers, T. T. (2015). Iterative Lasso: An even-handed approach to whole brain multivariate pattern analysis. 
<i>
Poster presented at the Neuroimaging, Computational Neuroscience and Neuroengineering Workshop, Madison, WI.
</i>
</p>


<b>Other ongoing MVPA projects: </b>
<br>
<a href = "https://github.com/QihongL/ECoG_animacyDecoding">Animacy </a> using human ECoG - with Chris Cox, Tim Rogers, Matt Lambon Ralph. 

<br>
<a href = "https://github.com/QihongL/fMRI_motionDecoding">2D/3D motion percept</a> using human fMRI - with Andrew Haun, Bas Rokers, Tim Rogers. 

 

<br><br><br>

<center><a href="#top">Go to top</a></center>


      </section>
      <footer>
        <p><small>Hosted on GitHub Pages<br>
        Theme by <a href="https://github.com/orderedlist">orderedlist</a>
        </small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
